# -*- coding: utf-8 -*-
"""extract_features3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZeZLpR4Jj4i5E3O7m4OfUtPDCbbX-hEW
"""

from google.colab import drive
drive.mount('/content/drive')

#MERTをクローン化
!git clone https://github.com/yizhilll/MERT.git

# インストール
!pip install datasets
!pip install torchaudio
!pip install nnAudio
!pip install pydub
!apt-get update && apt-get install -y ffmpeg

# インポート
import torch
import torchaudio.transforms as T
from transformers import Wav2Vec2FeatureExtractor, AutoModel
from datasets import load_dataset
import pandas as pd
import numpy as np
import os
import shutil
import glob
import csv

# 楽曲の処理前のみ実行
# 楽曲保存用のファイルを作成
#os.makedirs('/content/drive/MyDrive/data/custom_audio_dataset_manifest', exist_ok=True)
#os.makedirs('/content/drive/MyDrive/data/custom_audio_dataset', exist_ok=True)

# 楽曲の長さを指定する場合の設定
# Cut_mp3作成前だけ実行
#wav_files = glob.glob("/content/drive/MyDrive/Dataset_cut_wav*.wav")
#mp3_files = glob.glob("/content/drive/MyDrive/MusicData/**/*.mp3", recursive=True)  # 元のmp3ファイルのパス
#output_dir = "/content/drive/MyDrive/Cut_mp3"  # 出力先のディレクトリ

# 重複を排除
#mp3_files = list(set(mp3_files))

#output_dir = "/content/drive/MyDrive/Cut_mp3"  # 出力先のディレクトリ
#os.makedirs(output_dir, exist_ok=True)  # 出力先のディレクトリが存在しない場合は作成
#for mp3_file in mp3_files:
#    # 出力ファイルパス
#    filename = os.path.basename(mp3_file)
#    output_path = os.path.join(output_dir, filename)

    # 既にファイルが存在する場合はスキップ
    #if os.path.exists(output_path):
    #    print(f"ファイルは既に存在します: {output_path}. スキップします。")
    #    continue

    # mp3ファイルを読み込む
#    try:
#        audio = AudioSegment.from_mp3(mp3_file)

        # 0秒から120秒までの120秒間を抽出する
#        start_time = 0 * 1000  # 開始時間（ミリ秒）
#        end_time = 120 * 1000  # 終了時間（ミリ秒）
#        segment = audio[start_time:end_time]

        # 元のファイル名を取得
#        filename = os.path.basename(mp3_file)

        # 抽出した部分を新しいmp3ファイルとして保存する
#        output_path = os.path.join(output_dir, filename)  # 元のファイル名を使用
#        segment.export(output_path, format="mp3")
#    except CouldntDecodeError as e:
#        print(f"ファイルのデコード中にエラーが発生しました: {mp3_file}. スキップします。")
#        print(e)
#        continue

# サンプリングレートを24000Hzまで圧縮する。これより下では、custom_audio_dataset_manifestからmp3を読み込んでいる
#!python /content/MERT/scripts/prepare_manifest.py --root-dir /content/drive/MyDrive/MusicData \
#      --target-rate 24000 --converted-root-dir /content/drive/MyDrive/data/custom_audio_dataset \
#      --out-dir /content/drive/MyDrive/data/custom_audio_dataset_manifest --extension mp3

# モデルと出力するディレクトリ、シャードとその開始地点の設定
# RAMの節約のためにバッチサイズごとに特徴抽出 → csv保存を行っている。不要であれば削除
# 380,383,387,391,494は特徴抽出不可
model_name = "m-a-p/MERT-v1-95M"  # 使用するモデル名、v1-330Mへの変更も可能
data_dir = "/content/drive/MyDrive/data/custom_audio_dataset"  # オーディオデータのディレクトリ
output_file = "/content/drive/MyDrive/Dataset/features_MERT-v1-95M.csv"  # 出力ファイル名
sampling_rate = 24000  # オーディオのサンプルレート
files_per_shard = 10 # 分割するファイル数
start_index = 0 # 再開するファイルのインデックス
batch_size = 10  # バッチサイズ（10件ごとに保存）

# モデルとプロセッサ、データセットの読み込み
model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name, trust_remote_code=True)
dataset = load_dataset("audiofolder", data_dir=data_dir, split="train")

# ファイル分割の設定、分割しない場合はスキップ
# シャードの数を計算
num_shards = len(dataset) // files_per_shard + (len(dataset) % files_per_shard > 0)

## シャードディレクトリを作成
shard_dir = "/content/drive/MyDrive/data/custom_audio_dataset_shards4"  # シャードディレクトリのパス
os.makedirs(shard_dir, exist_ok=True)

## ファイルをシャードに分割 2分40秒
#for shard_index in range(num_shards):
#    shard_path = os.path.join(shard_dir, f"shard_{shard_index}")
#    os.makedirs(shard_path, exist_ok=True)
#    start_index = shard_index * files_per_shard
#    end_index = min((shard_index + 1) * files_per_shard, len(dataset))
#    for i in range(start_index, end_index):
#        # オーディオファイルのパスを取得
#        audio_path = dataset[i]["audio"]["path"]
#        # シャードディレクトリにファイルをコピー
#        shutil.copy(audio_path, shard_path)

#リサンプラーの設定
resample_rate = processor.sampling_rate
# make sure the sample_rate aligned
if resample_rate != sampling_rate:
    print(f'setting rate from {sampling_rate} to {resample_rate}')
    resampler = T.Resample(sampling_rate, resample_rate)
else:
    resampler = None

#ファイル名取得の設定
def extract_trackname(audio_path):
  """Extracts the track name from the audio file path."""
  audio_filename = os.path.basename(audio_path)
  return audio_filename.split(".")[0]  # Assuming trackname is before the file extension

"""# 特徴抽出"""

# 特徴抽出と圧縮、保存 1シャードあたり3時間くらい、楽曲の長さに依存する
total_file_index = 0  # 全体のファイルインデックスを初期化
for shard_index in range(num_shards):
    shard_dataset = load_dataset("audiofolder", data_dir=os.path.join(shard_dir, f"shard_{shard_index}"), split="train")
    compressed_features = []  # バッチごとのデータを保存するリスト
    for i, item in enumerate(shard_dataset):
        # start_index より前のファイルはスキップ
        total_file_index = shard_index * files_per_shard + i
        if total_file_index < start_index:  # 全体のファイルインデックスで比較
            continue

        # オーディオデータの読み込みとリサンプリング
        if resampler is None:
            input_audio = item["audio"]["array"]
        else:
            input_audio = resampler(torch.from_numpy(item["audio"]["array"]))

        # 特徴抽出
        inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)

       # すべての層の特徴を取得 (1~13層目)
       # np.meanでtimestepを平均化している。平均を使用しない場合は、特徴の保存形式を変更する必要がある。
       # 例えば１つのcsvに1つの楽曲を保存や、layer_timestep_featureNのような保存形式する。
       # １楽曲あたりのデータ量も多いため、取り扱いには注意
        all_layer_features = [np.mean(outputs.hidden_states[layer_index].numpy(), axis=0) for layer_index in range(13)]

        row_data = {'trackname': extract_trackname(item["audio"]["path"])}  # 音楽ファイル名

        # すべての層の特徴を1行に保存
        # 保存方法の変更はこの部分を変更
        for layer_index, layer_features in enumerate(all_layer_features):
            row_data.update({f'{layer_index}_feature{feature_index}': value
                             for feature_index, value in enumerate(layer_features)})

        compressed_features.append(row_data)

        # バッチサイズごとにCSVファイルに保存
        if (i + 1) % batch_size == 0:
            compressed_df = pd.DataFrame(compressed_features)
            compressed_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
            compressed_features = []  # バッチデータを初期化
            print(f"shard_{shard_index} の保存が完了しました")

    # シャードの最後のバッチを保存 (バッチサイズに満たない場合)
    if compressed_features:
        compressed_df = pd.DataFrame(compressed_features)
        compressed_df.to_csv(output_file, mode='a', header=False, index=False)

print(f"特徴抽出と圧縮が完了し、{output_file} に保存されました。")


## 以下はMERTで実装されたCNN圧縮を試したコードなので、不要であれば削除

# 特徴抽出と圧縮、保存 1シャードあたり3時間くらい
total_file_index = 0  # 全体のファイルインデックスを初期化
for shard_index in range(num_shards):
    shard_dataset = load_dataset("audiofolder", data_dir=os.path.join(shard_dir, f"shard_{shard_index}"), split="train")
    compressed_features = []  # バッチごとのデータを保存するリスト
    for i, item in enumerate(shard_dataset):
        # start_index より前のファイルはスキップ
        total_file_index = shard_index * files_per_shard + i
        if total_file_index < start_index:  # 全体のファイルインデックスで比較
            continue

        # オーディオデータの読み込みとリサンプリング
        if resampler is None:
            input_audio = item["audio"]["array"]
        else:
            input_audio = resampler(torch.from_numpy(item["audio"]["array"]))

        # 特徴抽出
        inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)

        # ここに挿入 ---------------------------------------------------

        import torch.nn as nn
        import pandas as pd

        # CNN層を定義
        conv_layers = eval("[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2")  # 文字列をリストに変換
        cnn = nn.Sequential(
            *[
                nn.Conv1d(in_channels=conv_layers[i - 1][0] if i > 0 else 768,  # 最初の層はMERTの出力次元
                          out_channels=layer[0],
                          kernel_size=layer[1],
                          stride=layer[2])
                for i, layer in enumerate(conv_layers)
            ]
        )

        # MERTモデルの出力にCNN層を適用
        all_layer_features = []
        for layer_index in range(13):
            hidden_states = outputs.hidden_states[layer_index][0].numpy()  # (timesteps, 768)
            hidden_states = torch.tensor(hidden_states).unsqueeze(0)  # (1, timesteps, 768)
            compressed_features = cnn(hidden_states.transpose(1, 2)).transpose(1, 2)  # (1, compressed_timesteps, 512)
            compressed_features = compressed_features.squeeze(0).detach().numpy()  # (compressed_timesteps, 512)
            all_layer_features.append(compressed_features)

        # タイムステップごとに特徴を平均化
        compressed_features_mean = []
        for layer_index in range(13):
            compressed_features_mean.append(np.mean(all_layer_features[layer_index], axis=1))  # タイムステップごとに平均

        # 特徴を保存
        row_data = {'trackname': extract_trackname(item["audio"]["path"])}  # 音楽ファイル名

        for layer_index, layer_features in enumerate(compressed_features_mean):
            row_data.update({f'{layer_index}_feature{feature_index}': value
                                     for feature_index, value in enumerate(layer_features)})

        compressed_df = pd.DataFrame([row_data])  # DataFrameを作成
        compressed_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)  # CSVに保存

        # ------------------------------------------------------------

# 特徴抽出は1時間あたり10曲、全曲で68時間くらいかかる
